{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1f0KsQe0PN1"
      },
      "outputs": [],
      "source": [
        "# Importing necessary classes and modules\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import glob\n",
        "import zipfile\n",
        "import shutil\n",
        "import csv\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting Google Drive for access\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "5NaAKzex2o6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing files and directories in the specified path in Google Drive\n",
        "!ls drive/MyDrive/inteligencia_artificial/Banana_Ripening_Process"
      ],
      "metadata": {
        "id": "UEx6m9rf2sg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading a CSV file from a specified path in Google Drive using pandas\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/inteligencia_artificial/Banana_Ripening_Process/train_classes.csv\", index_col=0)\n",
        "\n",
        "# Printing the shape of the DataFrame\n",
        "print(df.shape)\n",
        "\n",
        "# Printing the columns of the DataFrame\n",
        "print(df.columns)\n",
        "\n",
        "# Displaying the first few rows of the DataFrame\n",
        "df.head()"
      ],
      "metadata": {
        "id": "du-yf0_9nx84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the directory containing images\n",
        "image_path = \"/content/drive/MyDrive/inteligencia_artificial/Banana_Ripening_Process/train\"\n",
        "\n",
        "# Variable to count the number of skipped images\n",
        "num_skipped = 0\n",
        "\n",
        "# Loop through each file in the directory\n",
        "for fname in os.listdir(image_path):\n",
        "    # Get the complete path of the file\n",
        "    fpath = os.path.join(image_path, fname)\n",
        "    try:\n",
        "        # Open the file\n",
        "        fobj = open(fpath, \"rb\")\n",
        "        # Check if the file is a JPEG image\n",
        "        is_jpg = tf.compat.as_bytes(\"JFIF\") in fobj.peek(10)\n",
        "    finally:\n",
        "        # Close the file\n",
        "        fobj.close()\n",
        "\n",
        "    # If the file is not a JPEG image, delete it and update the count\n",
        "    if not is_jpg:\n",
        "        num_skipped += 1\n",
        "        os.remove(fpath)\n",
        "\n",
        "# Print the number of deleted images\n",
        "print(\"Deleted %d images\" % num_skipped)"
      ],
      "metadata": {
        "id": "O8HsC6jYpKle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining image size\n",
        "image_size = (224, 224)\n",
        "\n",
        "# Defining batch size for training\n",
        "batch_size = 16\n",
        "\n",
        "# Number of classes in the dataset\n",
        "num_classes = 5\n",
        "\n",
        "# Number of channels in the image (e.g., 3 for RGB images)\n",
        "num_channels = 3\n",
        "\n",
        "# Creating input shape for the model\n",
        "input_shape = image_size + (num_channels,)\n",
        "\n",
        "# Parameters for image data generator\n",
        "params = {'dim': image_size,        # Dimension of the images\n",
        "          'batch_size': batch_size, # Batch size for training\n",
        "          'n_classes': num_classes,  # Number of classes\n",
        "          'n_channels': num_channels, # Number of channels\n",
        "          'shuffle': False}          # Shuffling the data during training\n"
      ],
      "metadata": {
        "id": "uAeuViN4pvYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory containing the source images\n",
        "IMAGE_SOURCE = \"/content/drive/MyDrive/inteligencia_artificial/Banana_Ripening_Process/train\"\n",
        "\n",
        "# Directory where images will be split into classes\n",
        "SPLIT_DIR = \"/content/drive/MyDrive/inteligencia_artificial/Banana_Ripening_Process/classes\"\n",
        "\n",
        "# Read the CSV file containing class labels\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/inteligencia_artificial/Banana_Ripening_Process/train_classes.csv\")\n",
        "\n",
        "# Extract the class label values\n",
        "values = df[[' freshripe', ' freshunripe', ' overripe', ' ripe', ' rotten', ' unripe']]\n",
        "\n",
        "# Determine the dominant class label for each image\n",
        "labels = values.idxmax(axis=1)\n",
        "\n",
        "# Print the values and labels of the first few rows\n",
        "print(values)\n",
        "print(labels[:5])\n",
        "\n",
        "# Get the unique classes\n",
        "classes = labels.unique()\n",
        "\n",
        "# Create directories for each class if they don't exist\n",
        "for class_name in classes:\n",
        "    os.makedirs(os.path.join(SPLIT_DIR, class_name), exist_ok=True)\n",
        "\n",
        "# Copy images to the respective directories based on their class\n",
        "for index, row in df.iterrows():\n",
        "    image_name = row[\"filename\"]\n",
        "    class_name = labels[index]\n",
        "    shutil.copy(os.path.join(IMAGE_SOURCE, image_name), os.path.join(SPLIT_DIR, class_name, image_name))"
      ],
      "metadata": {
        "id": "uJrRYFCD0Apo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the folder containing categorized images\n",
        "image_folder = \"/content/drive/MyDrive/inteligencia_artificial/Banana_Ripening_Process/classes\"\n",
        "\n",
        "# Paths to specific folders for each class\n",
        "freshripe_folder = os.path.join(image_folder, \" freshripe\")\n",
        "freshunripe_folder = os.path.join(image_folder, \" freshunripe\")\n",
        "overripe_folder = os.path.join(image_folder, \" overripe\")\n",
        "ripe_folder = os.path.join(image_folder, \" ripe\")\n",
        "rotten_folder = os.path.join(image_folder, \" rotten\")\n",
        "\n",
        "# File paths to each class folder\n",
        "freshripe_path = os.path.join(freshripe_folder, \"*.jpg\")\n",
        "freshunripe_path = os.path.join(freshunripe_folder, \"*.jpg\")\n",
        "overripe_path = os.path.join(overripe_folder, \"*.jpg\")\n",
        "ripe_path = os.path.join(ripe_folder, \"*.jpg\")\n",
        "rotten_path = os.path.join(rotten_folder, \"*.jpg\")\n",
        "\n",
        "# Retrieving filenames using glob for each class\n",
        "freshripe_filenames = glob.glob(freshripe_path)\n",
        "freshunripe_filenames = glob.glob(freshunripe_path)\n",
        "overripe_filenames = glob.glob(overripe_path)\n",
        "ripe_filenames = glob.glob(ripe_path)\n",
        "rotten_filenames = glob.glob(rotten_path)\n",
        "\n",
        "# Printing the number of filenames in each class\n",
        "print(len(freshripe_filenames), len(freshunripe_filenames), len(overripe_filenames), len(ripe_filenames), len(rotten_filenames))"
      ],
      "metadata": {
        "id": "OJvXj3sc0trp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary containing the sizes of each class\n",
        "class_sizes = {\n",
        "    \"freshripe\": len(freshripe_filenames),\n",
        "    \"freshunripe\": len(freshunripe_filenames),\n",
        "    \"overripe\": len(overripe_filenames),\n",
        "    \"ripe\": len(ripe_filenames),\n",
        "    \"rotten\": len(rotten_filenames)\n",
        "}\n",
        "\n",
        "# Finding the class with the minimum number of samples\n",
        "class_name = min(class_sizes, key=class_sizes.get)\n",
        "\n",
        "# Getting the number of samples in the smallest class\n",
        "min_samples = class_sizes[class_name]\n",
        "\n",
        "# Printing the minimum number of samples\n",
        "print(min_samples)"
      ],
      "metadata": {
        "id": "D_DOskG-WiGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating separate dataframes for each class\n",
        "df_freshripe = pd.DataFrame()\n",
        "df_freshunripe = pd.DataFrame()\n",
        "df_overripe = pd.DataFrame()\n",
        "df_ripe = pd.DataFrame()\n",
        "df_rotten = pd.DataFrame()\n",
        "\n",
        "# Trimming the image folder path from the filenames\n",
        "for i in range(len(freshripe_filenames)):\n",
        "    freshripe_filenames[i] = freshripe_filenames[i][len(image_folder)+1:]\n",
        "for i in range(len(freshunripe_filenames)):\n",
        "    freshunripe_filenames[i] = freshunripe_filenames[i][len(image_folder)+1:]\n",
        "for i in range(len(overripe_filenames)):\n",
        "    overripe_filenames[i] = overripe_filenames[i][len(image_folder)+1:]\n",
        "for i in range(len(ripe_filenames)):\n",
        "    ripe_filenames[i] = ripe_filenames[i][len(image_folder)+1:]\n",
        "for i in range(len(rotten_filenames)):\n",
        "    rotten_filenames[i] = rotten_filenames[i][len(image_folder)+1:]\n",
        "\n",
        "# Shuffling the filenames within each class\n",
        "random.shuffle(freshripe_filenames)\n",
        "random.shuffle(freshunripe_filenames)\n",
        "random.shuffle(overripe_filenames)\n",
        "random.shuffle(ripe_filenames)\n",
        "random.shuffle(rotten_filenames)\n",
        "\n",
        "# Assigning filenames and labels to each class's dataframe\n",
        "df_freshripe[\"filename\"] = freshripe_filenames[:min_samples]\n",
        "df_freshripe[\"label\"] = 0\n",
        "\n",
        "df_freshunripe[\"filename\"] = freshunripe_filenames[:min_samples]\n",
        "df_freshunripe[\"label\"] = 1\n",
        "\n",
        "df_overripe[\"filename\"] = overripe_filenames[:min_samples]\n",
        "df_overripe[\"label\"] = 2\n",
        "\n",
        "df_ripe[\"filename\"] = ripe_filenames[:min_samples]\n",
        "df_ripe[\"label\"] = 3\n",
        "\n",
        "df_rotten[\"filename\"] = rotten_filenames[:min_samples]\n",
        "df_rotten[\"label\"] = 4\n",
        "\n",
        "# Printing the lengths and first few rows of each dataframe\n",
        "print(len(df_freshripe), len(df_overripe), len(df_ripe), len(df_rotten))\n",
        "print(df_freshripe.head(5))\n",
        "print(df_freshunripe.head(5))\n",
        "print(df_overripe.head(5))\n",
        "print(df_ripe.head(5))\n",
        "print(df_rotten.head(5))"
      ],
      "metadata": {
        "id": "PiKT2v1tYsV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of iterations for dataset splitting\n",
        "num_iterations = 10\n",
        "\n",
        "# Distribution of data among train, validation, and test sets\n",
        "distribution = {\"train\": 0.70, \"val\": 0.15, \"test\": 0.15}\n",
        "\n",
        "# Calculating lengths for each split\n",
        "len_train = int(min_samples * distribution[\"train\"])\n",
        "len_val = int(min_samples * distribution[\"val\"])\n",
        "len_test = int(min_samples * distribution[\"test\"])\n",
        "\n",
        "# Iterating over the dataset splits\n",
        "for i in range(num_iterations):\n",
        "\n",
        "  # Generating the training set\n",
        "  start = i * len_val\n",
        "  end = start + len_train\n",
        "  df_train = pd.concat([\n",
        "      df_freshripe[start:end],\n",
        "      df_freshunripe[start:end],\n",
        "      df_overripe[start:end],\n",
        "      df_ripe[start:end],\n",
        "      df_rotten[start:end]\n",
        "  ])\n",
        "\n",
        "  # Checking if the training set size is smaller than the expected size\n",
        "  if len(df_train) < (len_train*num_classes)-1:\n",
        "    start = 0\n",
        "    end = len_train - int(len(df_train)/ num_classes)\n",
        "    df_train2 = pd.concat([\n",
        "      df_freshripe[start:end],\n",
        "      df_freshunripe[start:end],\n",
        "      df_overripe[start:end],\n",
        "      df_ripe[start:end],\n",
        "      df_rotten[start:end]\n",
        "    ])\n",
        "\n",
        "    df_train = pd.concat([df_train, df_train2])\n",
        "\n",
        "  # Printing information about the training set\n",
        "  print(i, start, end, len(df_train))\n",
        "\n",
        "  # Generating the validation set\n",
        "  start = end\n",
        "  if start >= min_samples-1:\n",
        "    start = 0\n",
        "  end = start + len_val\n",
        "  df_val = pd.concat([\n",
        "      df_freshripe[start:end],\n",
        "      df_freshunripe[start:end],\n",
        "      df_overripe[start:end],\n",
        "      df_ripe[start:end],\n",
        "      df_rotten[start:end]\n",
        "  ])\n",
        "  print(i, start, end, len(df_val))\n",
        "\n",
        "  # Generating the test set\n",
        "  start = end\n",
        "  if start >= min_samples-1:\n",
        "    start = 0\n",
        "  end = start + len_test\n",
        "  df_test = pd.concat([\n",
        "      df_freshripe[start:end],\n",
        "      df_freshunripe[start:end],\n",
        "      df_overripe[start:end],\n",
        "      df_ripe[start:end],\n",
        "      df_rotten[start:end]\n",
        "  ])\n",
        "  print(i, start, end, len(df_test))\n",
        "\n",
        "  # Creating filenames for the splits\n",
        "  train_filename = \"train_ds_\" + str(i) + \".csv\"\n",
        "  val_filename = \"val_ds_\" + str(i) + \".csv\"\n",
        "  test_filename = \"test_ds_\" + str(i) + \".csv\"\n",
        "\n",
        "  # Shuffling the dataframes\n",
        "  df_train = df_train.sample(frac=1)\n",
        "  df_val = df_val.sample(frac=1)\n",
        "  df_test = df_test.sample(frac=1)\n",
        "\n",
        "  # Saving the dataframes to CSV files\n",
        "  df_train.to_csv(train_filename)\n",
        "  df_val.to_csv(val_filename)\n",
        "  df_test.to_csv(test_filename)\n",
        "\n",
        "  # Printing a separator line\n",
        "  print(\"-\"*60)"
      ],
      "metadata": {
        "id": "I4a8i2HdbLk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(num_iterations):\n",
        "  train_filename = \"train_ds_\" + str(i) + \".csv\"\n",
        "  val_filename = \"val_ds_\" + str(i) + \".csv\"\n",
        "  test_filename = \"test_ds_\" + str(i) + \".csv\"\n",
        "\n",
        "  df_train = pd.read_csv(train_filename)\n",
        "  df_val = pd.read_csv(val_filename)\n",
        "  df_test = pd.read_csv(test_filename)\n",
        "\n",
        "  print(df_train.groupby([\"label\"])[\"label\"].count())\n",
        "  print(df_val.groupby([\"label\"])[\"label\"].count())\n",
        "  print(df_test.groupby([\"label\"])[\"label\"].count())\n",
        "  print(\"-\"*60)\n",
        "  print()"
      ],
      "metadata": {
        "id": "ZsEXJbSfdXQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the iteration number\n",
        "i = 0\n",
        "\n",
        "# Creating filenames for the datasets\n",
        "train_filename = \"train_ds_\" + str(i) + \".csv\"\n",
        "val_filename = \"val_ds_\" + str(i) + \".csv\"\n",
        "test_filename = \"test_ds_\" + str(i) + \".csv\"\n",
        "\n",
        "# Reading the data from the CSV files into dataframes\n",
        "df_train = pd.read_csv(train_filename)\n",
        "df_val = pd.read_csv(val_filename)\n",
        "df_test = pd.read_csv(test_filename)"
      ],
      "metadata": {
        "id": "cjDQRRihdpnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an empty dictionary to store the partitions\n",
        "partition = {}\n",
        "\n",
        "# Storing the filenames for the training, validation, and test sets\n",
        "partition[\"train\"] = list(df_train[\"filename\"])\n",
        "partition[\"val\"] = list(df_val[\"filename\"])\n",
        "partition[\"test\"] = list(df_test[\"filename\"])\n",
        "\n",
        "# Printing the dictionary containing the partitions\n",
        "print(partition)"
      ],
      "metadata": {
        "id": "u6PzZEtXdvdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an empty dictionary to store the filename-label mapping\n",
        "labels = {}\n",
        "\n",
        "# Combining all dataframes into a single dataframe\n",
        "df_all = pd.concat([df_train, df_val, df_test])\n",
        "\n",
        "# Iterating through the combined dataframe to populate the 'labels' dictionary\n",
        "for index, row in df_all.iterrows():\n",
        "    filename = row[\"filename\"]\n",
        "    label = row[\"label\"]\n",
        "    labels[filename] = label\n",
        "\n",
        "# Printing the dictionary containing the filename-label mapping\n",
        "print(labels)"
      ],
      "metadata": {
        "id": "LJN7SET6dxGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image(image_filename):\n",
        "    # Opens an image in RGB mode\n",
        "    im1 = Image.open(image_filename).convert(\"RGB\")\n",
        "\n",
        "    # Resizes the image to the specified image_size\n",
        "    im1 = im1.resize(image_size)\n",
        "\n",
        "    # Prints the type of the image\n",
        "    print(type(im1))\n",
        "\n",
        "    # Converts the image to a numpy array\n",
        "    image = np.asarray(im1)\n",
        "\n",
        "    # Converts the image array to type 'float32'\n",
        "    image = np.array(image, dtype='float32')\n",
        "\n",
        "    # Normalizes the pixel values to be in the range [0, 1]\n",
        "    image = image / 255.0\n",
        "\n",
        "    # Prints the shape of the resulting image\n",
        "    print(image.shape)\n",
        "\n",
        "    # Returns the normalized image\n",
        "    return image"
      ],
      "metadata": {
        "id": "5QI_EgX1VXfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataGenerator(keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "\n",
        "    def __init__(self, list_IDs, labels, batch_size=128, dim=(180,180), n_channels=3,\n",
        "                 n_classes=2, shuffle=True):\n",
        "        'Initialization'\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.labels = labels\n",
        "        self.list_IDs = list_IDs\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(list_IDs_temp)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        'Generates data containing batch_size samples'  # X : (n_samples, *dim, n_channels)\n",
        "        # Initialization\n",
        "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
        "        y = np.empty((self.batch_size), dtype=int)\n",
        "\n",
        "        # Generate data\n",
        "        for i, ID in enumerate(list_IDs_temp):\n",
        "            # Store sample\n",
        "            # X[i,] = np.load('data/' + ID + '.npy')\n",
        "            image_filename = os.path.join(image_folder, ID)\n",
        "            X[i,] = get_image(image_filename)\n",
        "\n",
        "            # Store class\n",
        "            y[i] = self.labels[ID]\n",
        "\n",
        "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
      ],
      "metadata": {
        "id": "Z1E3dxEvVane"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a DataGenerator instance for the training set\n",
        "train_generator = DataGenerator(partition['train'], labels, **params)\n",
        "\n",
        "# Creating a DataGenerator instance for the validation set\n",
        "val_generator = DataGenerator(partition['val'], labels, **params)\n",
        "\n",
        "# Creating a DataGenerator instance for the test set\n",
        "test_generator = DataGenerator(partition['test'], labels, **params)"
      ],
      "metadata": {
        "id": "l7ImwEnwVejJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of epochs for training\n",
        "epochs = 3\n",
        "\n",
        "# Setting up the callback for early stopping\n",
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
        "]"
      ],
      "metadata": {
        "id": "kJO2IXXDVhSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing the keras-tuner package for hyperparameter tuning\n",
        "!pip install -q -U keras-tuner"
      ],
      "metadata": {
        "id": "cwgr-G4NVj8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the keras_tuner library\n",
        "import keras_tuner as kt"
      ],
      "metadata": {
        "id": "YF80DU7nVmAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_builder(hp):\n",
        "    # Choosing the model type\n",
        "    hp_model_type = hp.Choice(\n",
        "        \"model_type\", [\"EfficientNetV2L\", \"ResNet152V2\", \"DenseNet201\"], default=\"EfficientNetV2L\"\n",
        "    )\n",
        "\n",
        "    # Choosing the learning rate\n",
        "    hp_learning_rate = hp.Choice(\n",
        "        'learning_rate', values=[1e-2, 1e-3, 1e-4, 1e-5]\n",
        "    )\n",
        "\n",
        "    # Choosing the optimizer\n",
        "    hp_optimizer = hp.Choice(\n",
        "        \"optimizer\", values=[\"Adadelta\", \"RMSprop\", \"Adam\"], default=\"Adam\"\n",
        "    )\n",
        "\n",
        "    # Configuring the model based on the chosen hyperparameters\n",
        "    with hp.conditional_scope(\"model_type\", [\"EfficientNetV2L\"]):\n",
        "        if hp_model_type == \"EfficientNetV2L\":\n",
        "            model = tf.keras.applications.EfficientNetV2L(\n",
        "                include_top=True,\n",
        "                weights=None,\n",
        "                input_shape=input_shape,\n",
        "                classes=num_classes,\n",
        "                classifier_activation=\"softmax\",\n",
        "            )\n",
        "\n",
        "    with hp.conditional_scope(\"model_type\", [\"ResNet152V2\"]):\n",
        "        if hp_model_type == \"ResNet152V2\":\n",
        "            model = tf.keras.applications.ResNet152V2(\n",
        "                include_top=True,\n",
        "                weights=None,\n",
        "                input_shape=input_shape,\n",
        "                classes=num_classes,\n",
        "                classifier_activation=\"softmax\",\n",
        "            )\n",
        "\n",
        "    with hp.conditional_scope(\"model_type\", [\"DenseNet201\"]):\n",
        "        if hp_model_type == \"DenseNet201\":\n",
        "            model = tf.keras.applications.DenseNet201(\n",
        "                include_top=True,\n",
        "                weights=None,\n",
        "                input_shape=input_shape,\n",
        "                classes=num_classes,\n",
        "                classifier_activation=\"softmax\",\n",
        "            )\n",
        "\n",
        "    with hp.conditional_scope(\"optimizer\", [\"Adadelta\"]):\n",
        "        if hp_optimizer == \"Adadelta\":\n",
        "            optimizer = keras.optimizers.Adadelta(learning_rate=hp_learning_rate)\n",
        "\n",
        "    with hp.conditional_scope(\"optimizer\", [\"RMSprop\"]):\n",
        "        if hp_optimizer == \"RMSprop\":\n",
        "            optimizer = keras.optimizers.RMSprop(learning_rate=hp_learning_rate)\n",
        "\n",
        "    with hp.conditional_scope(\"optimizer\", [\"Adam\"]):\n",
        "        if hp_optimizer == \"Adam\":\n",
        "            optimizer = keras.optimizers.Adam(learning_rate=hp_learning_rate)\n",
        "\n",
        "    # Compiling the model with the chosen optimizer, loss function, and metrics\n",
        "    model.compile(\n",
        "        optimizer=hp_optimizer,\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "KHVyEy6XVouF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing a Hyperband tuner\n",
        "tuner = kt.Hyperband(\n",
        "    model_builder,  # The model_builder function defined earlier\n",
        "    objective='val_accuracy',  # The objective to optimize for\n",
        "    max_epochs=100,  # The maximum number of epochs to train the model\n",
        "    factor=3,  # Reduction factor for the number of epochs and number of models\n",
        "    overwrite=True,  # Whether to overwrite the results of a previous run\n",
        "    directory='my_dir',  # Directory to store the tuning results\n",
        "    project_name='intro_to_kt'  # Name of the project\n",
        ")"
      ],
      "metadata": {
        "id": "QS58aPcvVszA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initiating the hyperparameter search\n",
        "tuner.search(\n",
        "    train_generator,  # Data generator for training data\n",
        "    epochs=epochs,  # Number of epochs for training\n",
        "    callbacks=callbacks,  # Callbacks for monitoring the training process\n",
        "    validation_data=val_generator,  # Data generator for validation data\n",
        ")"
      ],
      "metadata": {
        "id": "SeyrzV-bVvbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieving the best hyperparameters from the completed tuner\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "# Printing the optimal learning rate for the optimizer\n",
        "print(f\"\"\"\n",
        "The hyperparameter search is complete. The optimal learning rate for the optimizer\n",
        "is {best_hps.get('learning_rate')}.\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "bFrwa3hsV0fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the model with the best hyperparameters\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Compiling the model with the specified configuration\n",
        "model = tf.keras.applications.DenseNet201(\n",
        "          include_top=True,\n",
        "          weights=None,\n",
        "          input_shape=input_shape,\n",
        "          classes=num_classes,\n",
        "          classifier_activation=\"softmax\",\n",
        "      )\n",
        "\n",
        "model.compile(\n",
        "  optimizer=keras.optimizers.Adadelta(learning_rate=1e-03),\n",
        "  loss=\"categorical_crossentropy\",\n",
        "  metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "# Training the model with the data generators and hyperparameters\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=100,\n",
        "    callbacks=callbacks,\n",
        "    validation_data=val_generator,\n",
        "    initial_epoch=0\n",
        ")\n",
        "\n",
        "# Obtaining the best epoch based on validation accuracy\n",
        "val_acc_per_epoch = history.history['val_accuracy']\n",
        "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
        "print('Best epoch: %d' % (best_epoch,))"
      ],
      "metadata": {
        "id": "7f9dJIB8V6m5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the path for saving the trained model\n",
        "model_save_path = '/content/drive/MyDrive/inteligencia_artificial/Banana_Ripening_Process/model2.h5'\n",
        "\n",
        "# Saving the model in .h5 format\n",
        "model.save(model_save_path)\n",
        "\n",
        "# Also saving the model in .keras format\n",
        "model.save('/content/drive/MyDrive/inteligencia_artificial/Banana_Ripening_Process/model2.keras')\n",
        "\n",
        "# Printing a message to confirm that the model has been saved\n",
        "print(f'Model saved in .keras and .h5')"
      ],
      "metadata": {
        "id": "ATBHTRU2gVMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the necessary library for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Setting the label for the x-axis\n",
        "plt.xlabel(\"# epoch\")\n",
        "\n",
        "# Setting the label for the y-axis\n",
        "plt.ylabel(\"Loss Magnitude\")\n",
        "\n",
        "# Plotting the loss over the epochs\n",
        "plt.plot(history.history[\"loss\"])"
      ],
      "metadata": {
        "id": "sRV_HFN195kl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}